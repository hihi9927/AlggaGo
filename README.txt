안녕하세용 
변경 사항에 대해 형님들의 이해를 돕도록 텍스트 파일 하나 팠습니다

강화 학습 방식의 특징
1. 나와 상대를 각각 따로 구축한다. (왜냐하면 과거의 나와 싸우는 방식은 비효과적(이라고 함) 왜냐하면 이미 전에 내가 학습했던 데이터이므로
유의미한 성능 개선을 이루어 내기 쉽지 않으며 과적합될 여지도 많다.)
2. 나와 상대는 모두 rule-based 모델에서 시작한다. (직관적이면서 모델이 빠르게 보상을 얻으며 학습할 수 있도록) 
--> 다만 이것은 근본적인 좋음이 아니기에 이를 향할 수 있도록 학습을 전개해 나가면서 점차 방향과 힘의 오차 범위를 증가시킨다. 
--> 현재 rule-based 모델은 index 선택이 랜덤인데 이를 가중치를 계산하는 방식으로 적용하여 학습할 수 있도록 한다. 
(왜냐하면 index 랜덤으로 하면 돌과 돌 사이에 다른 돌이 끼어 있어도 인지하지 못함)
3. 이때의 보상함수는 현재 rule-based가 취하는 행동보다 더 좋을 수 있는 다양한 상황을 고려해야 한다. 
- 1타 2피 / 1타 3피 
- 2대 1로 지고 있는 상황에서 돌 중간으로 끼워 맞추기 
- 그 외 더 ? 
4. 강화학습 방식은 GAN을 착안하여 설계한다. 
--> : 생성자와 판별자가 서로 경쟁하며 데이터를 생산하는 방식 
--> 둘다 각자 구축했으므로 내가 오차범위 조금 더 가지고 학습 --> 내가 이기면 이제 상대가 나보다 오차범위 조금 더 가지고 학습 -
-> 상대가 이기면 이제 내가 ... (그래서 각 모델을 만들 때 흰돌과 백돌을 모두 할 수 있도록 환경을 조성해야 한다.)

참고: 나와 상대는 모두 흑돌과 백돌을 할 수 있다. (형평성을 위해)
보상/ 가중치 고려: 돌 사이에 돌, 1타 2~피, (중간에 끼기)
엔트로피 뭐가 적절? 

이상 끝. 