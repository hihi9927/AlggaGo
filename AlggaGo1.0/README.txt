학습 방법 

1. 최종 고안
(1) rule-based 모델을 만든다. 여기서 rule-based는 자신의 돌이 상대 돌을 정확한 각도로 최대 힘으로 
쏘는 것이다. 

(2) 백돌과 흑돌 모델 모두 신경망 모델을 만들고 초기 가중치를 rule-based처럼 행동하도록 할당한다. 
- 이제 흑돌과 백돌 모두 현재는 rule-based로 행동하지만 학습할 수 있는 환경이 만들어진다. 

(3) 하지만 이것은 '근본적으로 좋음'의 모델이 아닐 것이다. 무조건 1타 1피밖에 하지 못한다. 
그리고 때로는 빗겨치는 것이 더 좋을 수도 있다. 그래서 엔트로피를 점차 증가시키는 방향으로 학습 방법을 설정하였다. 
그리고 GAN 방법에 영감을 받았다. 아래 표를 보면, 

|    (엔트로피)     |  백돌(A)  |   흑돌(B)   |
|-----------------|-----------|------------|
| 백돌이 이길 때까지 |   0.05    |     0      |
| 흑돌이 이길 때까지 |   0.05    |    0.10    |
| 백돌이 이길 때까지 |   0.15    |    0.10    |

- 먼저 A가 엔트로피 0.05를 가진다. 그리고 엔트로피가 0인 B를 이길 때까지 학습한다. 만약 어떤 스테이지에서 
모델 A가 모델 B를 이긴다면, 이제 모델 B의 엔트로피를 0.10으로 올린 뒤 이제 모델 B가 모델 A를 이길 때까지 학습한다. 
이는 B가 엔트로피 0.50이 될 때까지 학습한다. 이를 통해 잘하면서 다양한 탐험을 좋은 방향으로 마친 모델을 만들 수 
있었다. 

2. 1차 고안 
(1) 우리는 백돌을 플레이하는 ai를 만들고 싶었다. 하지만 그러기 위해서는 흑돌 모델도 만들어야 했다. 무조건이라고는 말을
못하겠지만 흑돌 모델이 움직이고 또 잘해야 그 상황에서 이기기 위해 학습해서 더 잘하게 될 것이라고 생각했다. 

(2) 하지만 우리는 두 개의 모델을 모두 만드는 것이 비효율적이라고 생각하였다. 왜냐하면 흑돌 모델 만든 것은 실질적으로
활용을 안하기 때문이다. 그래서 우리는 과거의 나와 싸우는 방식을 고려하게 되었다. 

(3) 이에 아래 표와 같이 학습한다. 0~20번째 학습까지 상대 흑돌은 랜덤으로 행동한다. 이후 백돌은 이어서 학습한다.
하지만 상대가 바뀐다. 이제 백돌은 과거의 자신과 싸우게 된다. 이런 식으로 학습하면서 발전해나가도록 설계하였다. 

|           |   백돌    |        흑돌        |
|-----------|-----------|-------------------|
| 스테이지 1 |   0~20    |        랜덤        |
| 스테이지 2 |  20~100   | 스테이지1에서의 백돌 |
| 스테이지 3 | 100~1000  | 스테이지2에서의 백돌 |

(4) 하지만 성과가 마음에 들지 않았다. 우리는 그 원인으로 백돌 모델과 흑돌 모델이 모두 초기에 바보이기 때문이라는 
분석을 한 뒤 최종 고안을 하였다. 